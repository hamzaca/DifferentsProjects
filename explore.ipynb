{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f40fb04-4c2d-49e0-96cd-afedf3718b68",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "Detect disaters from people's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c5e290-e119-4112-8593-1d30ed457344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f46e7-8232-4719-abe4-d659d5294293",
   "metadata": {},
   "source": [
    "### Load Data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f221b0e-f1a1-48a3-b58e-2d0790fce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "never_seen_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd911c9d-a81e-432c-9d19-c4149180cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d89301-7330-4230-b5c2-5c787489b9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   0     NaN      NaN   \n",
       "1   2     NaN      NaN   \n",
       "2   3     NaN      NaN   \n",
       "3   9     NaN      NaN   \n",
       "4  11     NaN      NaN   \n",
       "\n",
       "                                                                                               text  \n",
       "0                                                                Just happened a terrible car crash  \n",
       "1                                  Heard about #earthquake is different cities, stay safe everyone.  \n",
       "2  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all  \n",
       "3                                                          Apocalypse lighting. #Spokane #wildfires  \n",
       "4                                                     Typhoon Soudelor kills 28 in China and Taiwan  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "never_seen_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e54b7-bc49-40a7-96c8-36baa81d4cbd",
   "metadata": {},
   "source": [
    "### Preprocessing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56af2e57-0351-461f-8da7-afb1050d0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "acronyms_dict = pd.read_json(\"../data/acronyms.json\", typ = 'series')\n",
    "acronyms_list = list(acronyms_dict.keys())\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "\n",
    "contractions_dict = pd.read_json(\"../data/contractions.json\", typ = 'series')\n",
    "contractions_list = list(contractions_dict.keys())\n",
    "# Function to convert contractions in a text\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform(text):\n",
    "    # Removing other unicode characters\n",
    "    def remove_http(text):\n",
    "        http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "        pattern = r\"({})\".format(http) # creating pattern\n",
    "        return re.sub(pattern, \"\", text)\n",
    "    def remove_punctuation(text):\n",
    "        punct_str = string.punctuation\n",
    "        punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n",
    "        return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "    \n",
    "    # remove leading space\n",
    "    text = text.strip()\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # delete back to new line\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = remove_http(text)\n",
    "    text = text.replace(\"  \", \"\")\n",
    "    # text = remove_punctuation(text)\n",
    "    text = convert_acronyms(text)\n",
    "    text = convert_contractions(text)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512e0d4-c31e-484e-87e1-1cfa4e3bdc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520cd092-e4da-4079-add9-d53b94058d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text : Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all dob ain't\n",
      "preprocessed text : our deeds are the reason of this earthquake may allah forgive us all date of birth are not\n"
     ]
    }
   ],
   "source": [
    "text_example = train_df[\"text\"][0] + \" dob\" + \" ain't\"\n",
    "print(f\"raw text : {text_example}\\npreprocessed text : {transform(text_example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b86285-48dd-4d79-b85a-200be4757ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing applied\n",
    "train_df[\"text\"] = train_df.text.apply(lambda x : transform(x))\n",
    "never_seen_df[\"text\"] = never_seen_df.text.apply(lambda x : transform(x))\n",
    "# Get tweets and targets\n",
    "tweets, targets = list(train_df[\"text\"]), list(train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ae393-ae61-4f24-8710-5903f1bde401",
   "metadata": {},
   "source": [
    "## model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "995826a1-52fa-432e-ae29-3947f9aee89d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
    "    \n",
    "    \n",
    "class BERTClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            x = self.dropout(pooled_output)\n",
    "            logits = self.fc(x)\n",
    "            return logits\n",
    "        \n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            proba = torch.max(nn.Softmax(dim=1)(outputs), dim=1).values.tolist()[0]\n",
    "    return f\"{round(proba, 2)}% Disaster\" if preds.item() == 1 else f\"{round(proba, 2)}% No Disater\"\n",
    "\n",
    "def predict_batch(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9d42f-b0df-4e84-b012-7e840a932fae",
   "metadata": {},
   "source": [
    "## Training : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71c051e-f4d5-4d76-a069-4a5a1c47b0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(val_labels)=1523\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters\n",
    "bert_model_name = '../data/bert_large_uncased/'\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "batch_size = 25\n",
    "num_epochs = 3\n",
    "learning_rate =  1e-5 #\n",
    "test_ratio=0.2\n",
    "device = torch.device(\"cuda:0\")  \n",
    "\n",
    "# split the dataset to train and validation : \n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(tweets, targets, test_size=test_ratio, random_state=42)\n",
    "print(f\"{len(val_labels)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86aa32-fa41-4f01-bc2b-0fe7978e421a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17d7536-103f-4165-be53-2c357b18c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../data/bert_large_uncased/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\anaconda\\UE000074\\envs\\OCR_Hamza\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "# prepare dataset :\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# load Bert model\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea37f2a-6db9-410a-a360-d179eca0bff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Validation Accuracy: 0.8194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       874\n",
      "           1       0.79      0.79      0.79       649\n",
      "\n",
      "    accuracy                           0.82      1523\n",
      "   macro avg       0.82      0.82      0.82      1523\n",
      "weighted avg       0.82      0.82      0.82      1523\n",
      "\n",
      "Epoch 2/3\n",
      "Validation Accuracy: 0.8391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       874\n",
      "           1       0.87      0.73      0.79       649\n",
      "\n",
      "    accuracy                           0.84      1523\n",
      "   macro avg       0.85      0.83      0.83      1523\n",
      "weighted avg       0.84      0.84      0.84      1523\n",
      "\n",
      "Epoch 3/3\n",
      "Validation Accuracy: 0.8372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       874\n",
      "           1       0.85      0.76      0.80       649\n",
      "\n",
      "    accuracy                           0.84      1523\n",
      "   macro avg       0.84      0.83      0.83      1523\n",
      "weighted avg       0.84      0.84      0.84      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate(model, val_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5bbbf50-4080-40ca-85c9-65a88248f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction='0.9% No Disater'  text='leaving back to sf friday have not packed one single thing 911 emergency'\n",
      "prediction='0.68% Disaster'  text='thought it was a drought'\n",
      "prediction='0.59% No Disater'  text='tarekfatah you are burning in enemity of pakistan i m sure you will burn more and more'\n",
      "prediction='0.98% Disaster'  text='american weapons and support are fueling a bloody air war in yemen'\n",
      "prediction='0.97% Disaster'  text='to whom we shld ask tht from where this bldy pak terrorist has entered in our country'\n",
      "prediction='0.97% No Disater'  text='i liked a youtube video from centraluploadoh oh'\n",
      "prediction='0.99% Disaster'  text='six palestinians kidnapped in west bank hebron home demolished international middle east media center'\n",
      "prediction='0.97% Disaster'  text='globalwarming u s forest service says spending more than half of budget on fires รป_united states the agen'\n",
      "prediction='0.92% No Disater'  text=\"ashwilliams1 continues to be the best guest on iloveggletters this week's episode is bloody outrageous\"\n",
      "prediction='0.8% Disaster'  text='bay whale worries rescuers'\n",
      "prediction='0.97% Disaster'  text=\"check out 'nova nuclear meltdown disaster' pbs\"\n",
      "prediction='0.93% No Disater'  text='mikeparractor oh my god i cannot believe they killed off ross he was my favourite character with aaron dannybmiller i am devastated top acting'\n",
      "prediction='0.98% Disaster'  text=\"600 passengers abandoned at lrt station during tuesday's hailstorm yyc yycstorm abstorm\"\n",
      "prediction='0.94% No Disater'  text=\"i love dat lady ' crhedrys you nko ' foxy__siren oh finally jennifer aniston got married i am so happy for her ''\"\n",
      "prediction='0.9% No Disater'  text='janeannmorrison a former assembly candidate is still waiting for answers'\n",
      "prediction='0.99% Disaster'  text=\"wreckage 'conclusively confirmed' as from mh370 malaysia pm investigators and the families of those who we are\"\n",
      "prediction='0.93% No Disater'  text=\"eh it is the macy's thanksgiving parade tomorrow i shall be done i shall still electrocute your email now\"\n",
      "prediction='0.95% No Disater'  text='do you feel engulfed with low self image take the quiz'\n",
      "prediction='0.95% No Disater'  text='magicallester i will die i am actually being serious my heart will beat so fast it will fly out off my chest amp explode'\n",
      "prediction='0.92% No Disater'  text='wyattmccab you had throw a can of copenhagen wintergreen on the ground that would explode on your enemies and give them mouth cancer'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts =random.sample(list(never_seen_df['text']), k=20)\n",
    "\n",
    "for text in texts : \n",
    "    prediction = predict_sentiment(text, model, tokenizer, device, max_length=128)\n",
    "    print(f\"{prediction=}  {text=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610cda6-05b8-44b8-b9cf-a29f5650e176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee140f3-630d-4bf1-b69b-923fb7789bfb",
   "metadata": {},
   "source": [
    "## Submission : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f8ab72-572c-4963-b815-25ab8bda8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(never_seen_df['text'])\n",
    "\n",
    "preds = predict_batch(text, model, tokenizer, device)\n",
    "preditions = preds.tolist()\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = preditions\n",
    "os.remove(\"../data/submission.csv\")\n",
    "sample_submission.to_csv(\"../data/submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f942b8-d6e0-47a3-8ea0-6e06e8eb338a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
